{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3e1c01f-2df9-4bfb-9829-5e7d453e3fd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 50 files: ['snapshot_sym4_date58_pm.csv', 'snapshot_sym3_date30_pm.csv', 'snapshot_sym2_date50_am.csv', 'snapshot_sym2_date29_am.csv', 'snapshot_sym4_date16_am.csv', 'snapshot_sym0_date93_pm.csv', 'snapshot_sym3_date28_am.csv', 'snapshot_sym4_date42_am.csv', 'snapshot_sym1_date51_am.csv', 'snapshot_sym0_date44_pm.csv', 'snapshot_sym0_date96_pm.csv', 'snapshot_sym0_date76_am.csv', 'snapshot_sym3_date10_pm.csv', 'snapshot_sym1_date65_pm.csv', 'snapshot_sym3_date53_am.csv', 'snapshot_sym3_date97_pm.csv', 'snapshot_sym0_date20_pm.csv', 'snapshot_sym1_date9_pm.csv', 'snapshot_sym0_date54_pm.csv', 'snapshot_sym2_date98_am.csv', 'snapshot_sym3_date31_pm.csv', 'snapshot_sym0_date70_am.csv', 'snapshot_sym2_date45_am.csv', 'snapshot_sym3_date17_pm.csv', 'snapshot_sym4_date10_pm.csv', 'snapshot_sym3_date64_pm.csv', 'snapshot_sym0_date10_am.csv', 'snapshot_sym4_date18_am.csv', 'snapshot_sym0_date16_am.csv', 'snapshot_sym4_date16_pm.csv', 'snapshot_sym1_date44_am.csv', 'snapshot_sym0_date64_am.csv', 'snapshot_sym1_date101_am.csv', 'snapshot_sym0_date1_pm.csv', 'snapshot_sym4_date65_pm.csv', 'snapshot_sym1_date11_am.csv', 'snapshot_sym3_date14_pm.csv', 'snapshot_sym3_date80_am.csv', 'snapshot_sym2_date16_pm.csv', 'snapshot_sym2_date5_pm.csv', 'snapshot_sym3_date29_pm.csv', 'snapshot_sym3_date47_am.csv', 'snapshot_sym0_date90_pm.csv', 'snapshot_sym2_date71_pm.csv', 'snapshot_sym1_date35_am.csv', 'snapshot_sym3_date39_am.csv', 'snapshot_sym2_date67_pm.csv', 'snapshot_sym2_date88_am.csv', 'snapshot_sym1_date56_pm.csv', 'snapshot_sym1_date95_pm.csv']\n",
      "X_train shape: (66250, 100, 33), y_train shape: (66250,)\n",
      "X_test shape: (28450, 100, 33), y_test shape: (28450,)\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[200]\tvalid_0's multi_logloss: 0.305996\n",
      "Using device: cuda:0\n",
      "HybridLSTM(\n",
      "  (lstm): LSTM(33, 32, num_layers=2, batch_first=True, bidirectional=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (attention): Sequential(\n",
      "    (0): Linear(in_features=64, out_features=1, bias=True)\n",
      "    (1): Softmax(dim=1)\n",
      "  )\n",
      "  (lstm_fc): Linear(in_features=64, out_features=32, bias=True)\n",
      "  (lgb_fc): Linear(in_features=3, out_features=32, bias=True)\n",
      "  (final_fc): Linear(in_features=64, out_features=3, bias=True)\n",
      ")\n",
      "Epoch 0, Avg Loss: 0.8970544220858099\n",
      "Epoch 1, Avg Loss: 0.5370683290323235\n",
      "Epoch 2, Avg Loss: 0.3462510547927908\n",
      "Epoch 3, Avg Loss: 0.28822888244072903\n",
      "Epoch 4, Avg Loss: 0.27106785618890666\n",
      "Epoch 5, Avg Loss: 0.2646238359705362\n",
      "Epoch 6, Avg Loss: 0.2607913612168728\n",
      "Epoch 7, Avg Loss: 0.25916047167018574\n",
      "Epoch 8, Avg Loss: 0.2563335690143946\n",
      "Epoch 9, Avg Loss: 0.2534655402180771\n",
      "Epoch 10, Avg Loss: 0.25023040008590947\n",
      "Epoch 11, Avg Loss: 0.24513530627640978\n",
      "Epoch 12, Avg Loss: 0.23989026737489294\n",
      "Epoch 13, Avg Loss: 0.23313159987503038\n",
      "Epoch 14, Avg Loss: 0.227632057540205\n",
      "Epoch 15, Avg Loss: 0.22206207701368222\n",
      "Epoch 16, Avg Loss: 0.21600077401947332\n",
      "Epoch 17, Avg Loss: 0.2114779176514121\n",
      "Epoch 18, Avg Loss: 0.20689538216153627\n",
      "Epoch 19, Avg Loss: 0.20154174454654045\n",
      "Epoch 20, Avg Loss: 0.1979993506403043\n",
      "Epoch 21, Avg Loss: 0.1931614366622505\n",
      "Epoch 22, Avg Loss: 0.1900194225451661\n",
      "Epoch 23, Avg Loss: 0.1846105117767934\n",
      "Epoch 24, Avg Loss: 0.18070086111893524\n",
      "Epoch 25, Avg Loss: 0.17672189750841685\n",
      "Epoch 26, Avg Loss: 0.17234995193117833\n",
      "Epoch 27, Avg Loss: 0.1684283651843034\n",
      "Epoch 28, Avg Loss: 0.16506408698655464\n",
      "Epoch 29, Avg Loss: 0.16170314149967033\n",
      "Epoch 30, Avg Loss: 0.15829653019610518\n",
      "Epoch 31, Avg Loss: 0.1535705907349421\n",
      "Epoch 32, Avg Loss: 0.14988629885276772\n",
      "Epoch 33, Avg Loss: 0.14685868788294812\n",
      "Epoch 34, Avg Loss: 0.14271437770841663\n",
      "Epoch 35, Avg Loss: 0.14135915075489913\n",
      "Epoch 36, Avg Loss: 0.1610382560022089\n",
      "Epoch 37, Avg Loss: 0.14782368834759738\n",
      "Epoch 38, Avg Loss: 0.13887714390358868\n",
      "Epoch 39, Avg Loss: 0.13423656882236362\n",
      "Epoch 40, Avg Loss: 0.12924812719743684\n",
      "Epoch 41, Avg Loss: 0.1268329461281364\n",
      "Epoch 42, Avg Loss: 0.12427436665390909\n",
      "Epoch 43, Avg Loss: 0.12126227442362134\n",
      "Epoch 44, Avg Loss: 0.11941737906007693\n",
      "Epoch 45, Avg Loss: 0.11791148200688675\n",
      "Epoch 46, Avg Loss: 0.11311532387287\n",
      "Epoch 47, Avg Loss: 0.11001423484570272\n",
      "Epoch 48, Avg Loss: 0.10961388340615398\n",
      "Epoch 49, Avg Loss: 0.10735556189848189\n",
      "Training completed in 162.99 seconds\n",
      "Test Accuracy: 0.8563\n",
      "Model saved as hybrid_model_label_5.pth\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import time\n",
    "from multiprocessing import Pool\n",
    "import random\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# 保持原有的数据处理函数不变\n",
    "def process_window(args):\n",
    "    i, features_values, labels_values, WINDOW_SIZE, label_index = args\n",
    "    window_features = features_values[i:i+WINDOW_SIZE]\n",
    "    label_value = labels_values[i + WINDOW_SIZE + [5, 10, 20, 40, 60][label_index] - 1, label_index]\n",
    "    return (window_features, label_value)\n",
    "\n",
    "def process_single_file(file_path, label_index=0):\n",
    "    data = pd.read_csv(file_path)\n",
    "    data = data.sort_values('time')\n",
    "    \n",
    "    WINDOW_SIZE = 100\n",
    "    PREDICTION_STEPS = [5, 10, 20, 40, 60]\n",
    "    \n",
    "    data['time'] = data['time'].astype('datetime64[ns]').dt.hour\n",
    "    features = data.drop(columns=['date'])\n",
    "    labels = data[['label_5', 'label_10', 'label_20', 'label_40', 'label_60']]\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    features = pd.DataFrame(scaler.fit_transform(features), columns=features.columns)\n",
    "    \n",
    "    total_samples = len(data) - WINDOW_SIZE - PREDICTION_STEPS[label_index]\n",
    "    \n",
    "    features_values = features.values\n",
    "    labels_values = labels.values\n",
    "    \n",
    "    args_list = [(i, features_values, labels_values, WINDOW_SIZE, label_index) for i in range(total_samples)]\n",
    "    \n",
    "    with Pool() as p:\n",
    "        results = p.map(process_window, args_list)\n",
    "    X, y = zip(*results)\n",
    "    \n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    \n",
    "    return train_test_split(X, y, test_size=0.3, shuffle=False, random_state=42)\n",
    "\n",
    "def process_folder(folder_path, label_index=0, sample_size=None, random_seed=42):\n",
    "    all_X_train = []\n",
    "    all_y_train = []\n",
    "    all_X_test = []\n",
    "    all_y_test = []\n",
    "    \n",
    "    csv_files = [f for f in os.listdir(folder_path) if f.endswith('.csv')]\n",
    "    \n",
    "    random.seed(random_seed)\n",
    "    if sample_size is not None and sample_size < len(csv_files):\n",
    "        csv_files = random.sample(csv_files, sample_size)\n",
    "    \n",
    "    print(f\"Processing {len(csv_files)} files: {csv_files}\")\n",
    "    \n",
    "    for file_name in csv_files:\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        X_train, X_test, y_train, y_test = process_single_file(file_path, label_index)\n",
    "        \n",
    "        all_X_train.append(X_train)\n",
    "        all_y_train.append(y_train)\n",
    "        all_X_test.append(X_test)\n",
    "        all_y_test.append(y_test)\n",
    "    \n",
    "    final_X_train = np.concatenate(all_X_train, axis=0)\n",
    "    final_y_train = np.concatenate(all_y_train, axis=0)\n",
    "    final_X_test = np.concatenate(all_X_test, axis=0)\n",
    "    final_y_test = np.concatenate(all_y_test, axis=0)\n",
    "    \n",
    "    return final_X_train, final_X_test, final_y_train, final_y_test\n",
    "\n",
    "# 加载数据\n",
    "label_index = 0 \n",
    "folder_path = 'train_set'\n",
    "sample_size = 50\n",
    "\n",
    "X_train, X_test, y_train, y_test = process_folder(\n",
    "    folder_path, \n",
    "    label_index=label_index, \n",
    "    sample_size=sample_size\n",
    ")\n",
    "\n",
    "print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\")\n",
    "\n",
    "# 训练LightGBM模型并获取预测概率作为特征\n",
    "X_train_reshaped = X_train.reshape(X_train.shape[0], -1)\n",
    "X_test_reshaped = X_test.reshape(X_test.shape[0], -1)\n",
    "\n",
    "# LightGBM参数\n",
    "params = {\n",
    "    'objective': 'multiclass',\n",
    "    'num_class': 3,\n",
    "    'metric': 'multi_logloss',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'num_leaves': 31,\n",
    "    'learning_rate': 0.05,\n",
    "    'feature_fraction': 0.9,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 5,\n",
    "    'verbose': 0,\n",
    "    'seed': 42\n",
    "}\n",
    "\n",
    "num_round = 200\n",
    "train_data = lgb.Dataset(X_train_reshaped, label=y_train)\n",
    "test_data = lgb.Dataset(X_test_reshaped, label=y_test, reference=train_data)\n",
    "\n",
    "bst = lgb.train(params, \n",
    "               train_data, \n",
    "               num_round, \n",
    "               valid_sets=[test_data], \n",
    "               callbacks=[lgb.early_stopping(stopping_rounds=10)])\n",
    "\n",
    "# 获取LightGBM的预测概率\n",
    "lgb_train_probs = bst.predict(X_train_reshaped)\n",
    "lgb_test_probs = bst.predict(X_test_reshaped)\n",
    "\n",
    "# 修改LSTM模型以接受LightGBM特征\n",
    "class HybridLSTM(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, num_classes, lgb_feature_dim=3, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_dim, hidden_dim, num_layers,\n",
    "            batch_first=True, bidirectional=True\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.attention = nn.Sequential( \n",
    "            nn.Linear(hidden_dim*2, 1),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "        # 增加一个全连接层来处理LSTM输出\n",
    "        self.lstm_fc = nn.Linear(hidden_dim*2, hidden_dim)\n",
    "        # 处理LightGBM特征的层\n",
    "        self.lgb_fc = nn.Linear(lgb_feature_dim, hidden_dim)\n",
    "        # 最终分类层\n",
    "        self.final_fc = nn.Linear(hidden_dim*2, num_classes)\n",
    "    \n",
    "    def forward(self, x, lgb_features):\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        \n",
    "        attn_weights = self.attention(lstm_out)\n",
    "        context = torch.sum(attn_weights * lstm_out, dim=1)\n",
    "        \n",
    "        # LSTM分支\n",
    "        lstm_features = self.lstm_fc(context)\n",
    "        lstm_features = self.dropout(lstm_features)\n",
    "        \n",
    "        # LightGBM分支\n",
    "        lgb_features = self.lgb_fc(lgb_features)\n",
    "        \n",
    "        # 合并特征\n",
    "        combined = torch.cat([lstm_features, lgb_features], dim=1)\n",
    "        \n",
    "        return self.final_fc(combined)\n",
    "\n",
    "# 设置设备\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# 准备数据\n",
    "input_dim = 33\n",
    "hidden_dim = 32\n",
    "num_layers = 2\n",
    "num_epochs = 50\n",
    "num_classes = 3\n",
    "\n",
    "# 将LightGBM概率特征转换为tensor\n",
    "lgb_train_tensor = torch.from_numpy(lgb_train_probs).float().to(device)\n",
    "lgb_test_tensor = torch.from_numpy(lgb_test_probs).float().to(device)\n",
    "\n",
    "# 原始特征tensor\n",
    "X_train_tensor = torch.from_numpy(X_train).float().to(device)\n",
    "y_train_tensor = torch.from_numpy(y_train).long().to(device)\n",
    "X_test_tensor = torch.from_numpy(X_test).float().to(device)\n",
    "y_test_tensor = torch.from_numpy(y_test).long().to(device)\n",
    "\n",
    "# 创建混合模型\n",
    "model = HybridLSTM(input_dim, hidden_dim, num_layers, num_classes).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimiser = torch.optim.RAdam(model.parameters(), lr=0.001)\n",
    "\n",
    "print(model)\n",
    "\n",
    "# 创建DataLoader\n",
    "batch_size = 256\n",
    "# 使用TensorDataset来组合原始特征和LightGBM特征\n",
    "train_dataset = TensorDataset(X_train_tensor, lgb_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# 训练循环\n",
    "hist = np.zeros(num_epochs)\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch_x, batch_lgb, batch_y in train_loader:\n",
    "        optimiser.zero_grad()\n",
    "        outputs = model(batch_x, batch_lgb)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimiser.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    hist[epoch] = avg_loss\n",
    "    print(f\"Epoch {epoch}, Avg Loss: {avg_loss}\")\n",
    "\n",
    "training_time = time.time() - start_time\n",
    "print(f\"Training completed in {training_time:.2f} seconds\")\n",
    "\n",
    "# 评估模型\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(X_test_tensor, lgb_test_tensor)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    accuracy = (predicted == y_test_tensor).sum().item() / y_test_tensor.size(0)\n",
    "    print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# 保存模型\n",
    "torch.save(model.state_dict(), f\"hybrid_model_label_{[5,10,20,40,60][label_index]}.pth\")\n",
    "print(f\"Model saved as hybrid_model_label_{[5,10,20,40,60][label_index]}.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b5ef38-f87c-4bc8-b7fe-29bcbf28ce17",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
